
# Telegram-бот для типографии "Азарин" с AI-ассистентом

Это продвинутый Telegram-бот, разработанный на Kotlin для автоматизации консультаций в типографии "Азарин". Он использует локально развернутую большую языковую модель (LLM) для ведения осмысленного диалога с пользователями, а также предоставляет надежный пошаговый калькулятор для расчета стоимости продукции.


## Основные возможности

*   **Диалоговый AI:** Бот способен вести естественный диалог, понимать уточняющие вопросы и помнить предыдущие реплики пользователя.
*   **Надежный калькулятор:** Реализован пошаговый калькулятор с кнопками, который позволяет точно рассчитать стоимость стандартной продукции (значки, цифровая печать, резка материалов) на основе актуальных прайс-листов.
*   **Локальная LLM:** Вся обработка AI-запросов происходит на вашем оборудовании с помощью `llama.cpp`, что обеспечивает полную приватность и независимость от внешних сервисов.
*   **Двусторонний перевод:** Для повышения качества ответов LLM, запросы пользователей переводятся на английский язык, а ответы модели переводятся обратно на русский. Это происходит незаметно для пользователя благодаря локальному сервису `LibreTranslate`.
*   **Асинхронная обработка:** Запросы к LLM могут занимать время. Чтобы бот не "зависал", используется система очередей, позволяющая обрабатывать тяжелые задачи в фоновом режиме.
*   **Чистая архитектура:** Проект построен на принципах разделения ответственности, что упрощает его поддержку и расширение.

---

## Как это работает: Магия больших языковых моделей

Этот бот — прекрасный пример практического применения LLM. Давайте разберемся, как текст вопроса превращается в осмысленный ответ.

### Что такое LLM?

**Большая языковая модель (LLM)** — это нейронная сеть огромного размера (миллиарды параметров), обученная на гигантских объемах текста из интернета, книг и других источников. В процессе обучения она не просто запоминает текст, а учится выявлять статистические закономерности, грамматические структуры, смысловые связи и даже логические отношения между словами и понятиями.

По своей сути, LLM — это невероятно сложный "предсказатель следующего слова".

### От промпта к ответу: пошаговый процесс

Когда вы отправляете боту сообщение в режиме диалога, происходит следующее:

1.  **Формирование Промпта (Prompt Engineering):**
    Бот не просто отправляет ваш вопрос модели. Он конструирует специальный текстовый блок — **промпт**. В нашем боте он состоит из трех частей:
    *   **Системная инструкция (System Prompt):** Это "роль" или "личность", которую мы задаем модели. Например: _"Ты — вежливый и полезный ассистент типографии 'Азарин'. Отвечай коротко и по делу."_ Это заставляет модель генерировать ответы в нужном нам стиле и контексте.
    *   **История диалога (Context):** Бот добавляет в промпт несколько последних пар "вопрос-ответ", чтобы модель "помнила", о чем шла речь ранее.
    *   **Новый запрос пользователя (User Prompt):** Ваше последнее сообщение.

2.  **Токенизация:**
    Получив промпт, LLM не работает с ним как с цельным текстом. Она разбивает его на минимальные смысловые единицы — **токены**. Токен может быть словом, частью слова (например, "бот" и "а"), знаком препинания. Например, фраза "Привет, мир!" будет разбита на токены `["Привет", ",", " мир", "!"]`.

3.  **Генерация (Inference):**
    Это и есть магия. Модель анализирует последовательность токенов из промпта и, используя свои внутренние "знания" (веса нейронов), вычисляет вероятность того, какой токен должен идти следующим.
    *   Она генерирует самый вероятный токен (например, после "Какие у вас услуги?" токен "Мы" будет очень вероятен).
    *   Затем она добавляет этот новый токен к последовательности и повторяет процесс: анализирует уже "Какие у вас услуги? Мы" и предсказывает следующий токен ("предоставляем").
    *   Этот процесс повторяется токен за токеном, пока модель не сгенерирует специальный токен конца текста или не достигнет лимита по количеству токенов.

4.  **Обратная токенизация и постобработка:**
    Последовательность сгенерированных токенов собирается обратно в читаемый текст. Наш бот затем выполняет дополнительную очистку (убирает технические теги) и отправляет вам чистый ответ.

Таким образом, LLM не "думает" и не "понимает" в человеческом смысле. Она является невероятно мощным статистическим инструментом, который на основе контекста предсказывает наиболее подходящее продолжение текста, создавая иллюзию осмысленного диалога.

---

## Инструкция по запуску и настройке

Для запуска проекта на вашем компьютере (протестировано на Ubuntu) необходимо выполнить три основных шага: настроить локальную LLM, запустить сервис перевода и запустить самого бота.

### Шаг 1: Настройка локальной LLM (`llama.cpp`)

LLM будет работать на вашем CPU.

#### 1.1. Установка зависимостей
Откройте терминал и установите инструменты для сборки:
```bash
sudo apt update
sudo apt install -y git make g++
```

#### 1.2. Сборка `llama.cpp`
Мы скомпилируем `llama.cpp` из исходного кода, отключив все необязательные зависимости для максимальной стабильности.

```bash
# Перейдите в домашнюю директорию и скачайте исходный код
cd ~
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Очищаем предыдущие попытки сборки (на всякий случай)
rm -rf build

# Конфигурируем сборку с помощью CMake, отключая BLAS и Vulkan
cmake -B build -DGGML_BLAS=OFF -DGGML_VULKAN=OFF -DLLAMA_CURL=OFF

# Запускаем саму компиляцию
cmake --build build --config Release
```
После завершения в папке `~/llama.cpp/build/bin/` появится исполняемый файл `llama-cli`.

#### 1.3. Скачивание модели
Создадим папку для моделей и скачаем туда квантованную (сжатую) версию `Phi-3-mini`.

```bash
cd ~
mkdir llm_models
cd llm_models

# Скачиваем модель (размер ~2.2 ГБ)
wget -c "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
```

### Шаг 2: Запуск сервиса переводов (`LibreTranslate`)

Мы будем использовать Docker для простого и изолированного запуска.

#### 2.1. Установка Docker
Если Docker не установлен, выполните официальную инструкцию по установке:
```bash
# Установка необходимых пакетов
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Добавляем пользователя в группу docker, чтобы не использовать sudo
sudo usermod -aG docker ${USER}

# ВАЖНО: после этого нужно выйти из системы и войти снова.
```

#### 2.2. Запуск контейнера LibreTranslate
Запустите контейнер в фоновом режиме. Он будет автоматически стартовать вместе с системой. Мы загружаем только русские и английские языковые пакеты для экономии ресурсов.

```bash
docker run -d --name libretranslate --restart always -p 5000:5000 libretranslate/libretranslate --load-only "en,ru"
```
Чтобы убедиться, что сервис работает, откройте в браузере `http://localhost:5000`.

### Шаг 3: Настройка и запуск проекта

#### 3.1. Клонирование репозитория
```bash
git clone https://github.com/khnychenkoav/TelegramTypographyV2.0.git
cd TelegramTypographyV2.0
```

#### 3.2. Настройка путей и токена
Откройте проект в IntelliJ IDEA. Вам нужно отредактировать два файла:

1.  **`src/main/kotlin/utils/Constants.kt`**:
    Вставьте сюда токен вашего Telegram-бота.
    ```kotlin
    const val BOT_TOKEN = "ВАШ_ТЕЛЕГРАМ_ТОКЕН"
    ```

2.  **`src/main/kotlin/Main.kt`**:
    Укажите абсолютные пути к `llama-cli` и файлу модели, которые вы настроили на Шаге 1.
    ```kotlin
    // ...
    val llamaBinaryPath = "/home/имя_пользователя/llama.cpp/build/bin/llama-cli"
    val modelPath = "/home/имя_пользователя/llm_models/Phi-3-mini-4k-instruct-q4.gguf"
    // ...
    ```

#### 3.3. Запуск
Откройте файл `src/main/kotlin/Main.kt` и нажмите на зеленую стрелку "Run" рядом с функцией `main()`.

После инициализации бот будет готов к работе в Telegram.


---

## Полный план работ

**Статус проекта:** `Этап 1.1 завершен.`

---

### ✅ Этап 0: Базовая архитектура и интеграции - `Выполнено`

*   **[✔] Задача 0.1-0.6:** Создание скелета проекта, подключение к Telegram, реализация управления сессиями, интеграция с локальной LLM и сервисом переводов, реализация диалога с памятью.
    *   **Результат:** Бот обладает чистой архитектурой, умеет вести осмысленный диалог и готов к расширению функционала.

---

### ✅ Этап 1: Фундаментальный функционал (MVP) - `Частично выполнен`

На этом этапе доводим до ума самые важные для бизнеса функции.

*   **[✔] Задача 1.1: Надежный кнопочный калькулятор - `Выполнено`**
    *   **Описание:** Реализована система точного расчета стоимости по прайс-листам. Пользователь пошагово выбирает параметры заказа с помощью кнопок.
    *   **[✔] Подзадача 1.1.1:** Создана и отлажена структура прайс-листов (JSON) для всех основных продуктов.
    *   **[✔] Подзадача 1.1.2:** Реализован `PriceListProvider` для загрузки и парсинга прайсов.
    *   **[✔] Подзадача 1.1.3:** Реализован `CalculatorService` с надежной логикой расчета для всех заявленных продуктов (значки, цифровая печать, резка, резка с печатью).
    *   **[✔] Подзадача 1.1.4:** Реализована полная и корректная пошаговая логика в `ResponseHandler` для всех сценариев калькулятора.
    *   **Примерный срок:** `10-14 дней`.

*   **[ ] Задача 1.2: Интеграция LLM в калькулятор - `Предстоит`**
    *   **Описание:** Создание гибридной системы, где AI помогает пользователю, а жесткая логика по-прежнему отвечает за точность.
    *   **[ ] Подзадача 1.2.1:** Реализовать режим **"LLM-Анализатор"**: пользователь описывает заказ текстом, а LLM извлекает из текста параметры и передает их в `CalculatorService`.
    *   **[ ] Подзадача 1.2.2:** Реализовать режим **"LLM-Комментатор"**: после получения точного расчета от `CalculatorService`, LLM "оборачивает" его в красивый, человечный ответ с полезными советами.
    *   **[ ] Подзадача 1.2.3:** Обработка случаев, когда LLM не может распознать заказ или запрашиваемый продукт отсутствует в прайс-листе.
    *   **Примерный срок:** `5-7 дней`.

*   **[✔] Задача 1.3: Надежная связь с оператором**
    *   **Описание:** Реализация функции "Связаться с оператором", которая пересылает сообщение пользователя в специальный чат.
    *   **[✔] Подзадача 1.3.1:** Настроить ID чата операторов.
    *   **[✔] Подзадача 1.3.2:** Реализовать форматирование и пересылку сообщения в `ResponseHandler`.
    *   **Примерный срок:** `2 дня`.

*   **[ ] Задача 1.4: Интеграция и выбор удаленной LLM (GigaChat)**
    *   **Описание:** Добавление возможности использовать более мощную облачную модель для сложных задач.
    *   **[ ] Подзадача 1.4.1:** Добавить зависимости GigaChat SDK.
    *   **[ ] Подзадача 1.4.2:** Создать `GigaChatServiceImpl.kt`.
    *   **[ ] Подзадача 1.4.3:** Реализовать в интерфейсе бота (кнопки, команды) возможность переключения между локальной и удаленной моделью.
    *   **Примерный срок:** `4-5 дней`.

---

### ⏳ Этап 2: Улучшение пользовательского опыта (UX/UI) - `Предстоит, ~2 недели`

Делаем взаимодействие с ботом приятным и интуитивным.

*   **[ ] Задача 2.1: Визуальное обогащение ответов**
    *   **Описание:** Добавление изображений, Markdown-разметки и анимаций.
    *   **Примерный срок:** `3 дня`.

*   **[ ] Задача 2.2: "Приборка" в чате**
    *   **Описание:** Реализация удаления/редактирования старых сообщений и клавиатур для поддержания чистоты в диалоге.
    *   **Примерный срок:** `3 дня`.

*   **[ ] Задача 2.3: Расширение навигации и информационных разделов**
    *   **Описание:** Добавление кнопок и обработчиков для статических информационных разделов ("Адреса", "Требования к макетам" и т.д.).
    *   **Примерный срок:** `2 дня`.

---

### ⏳ Этап 3: Продвинутые AI-возможности - `Предстоит, ~4-5 недель`

Выходим за рамки простого ассистента.

*   **[ ] Задача 3.1: Обработка файлов и изображений**
    *   **Описание:** Научить бота принимать файлы (макеты) и изображения, пересылать их оператору. В перспективе — анализ изображений мультимодальной LLM.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 3.2: Интеграция с "интернетом" (поиск актуальной информации)**
    *   **Описание:** Дать боту возможность искать информацию в интернете для ответов на вопросы о трендах, новых материалах и т.д.
    *   **Примерный срок:** `7-10 дней`.

*   **[ ] Задача 3.3: Реализация пошагового брифа на дизайн**
    *   **Описание:** Создание интерактивного опросника, который проведет пользователя по всем вопросам из вашего брифа.
    *   **Примерный срок:** `5-7 дней`.

---

### ⏳ Этап 4: Администрирование и развертывание - `Предстоит, ~2 недели`

Подготовка к реальному использованию.

*   **[ ] Задача 4.1: Режим для сотрудников**
    *   **Описание:** Создание панели администратора с командами для просмотра статистики, логов и отправки массовых рассылок.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 4.2: Подготовка к развертыванию (Deployment)**
    *   **Описание:** Написание Dockerfile для всего приложения, настройка CI/CD для автоматической сборки и развертывания на сервере.
    *   **Примерный срок:** `3-5 дней`.

---

**Общий примерный срок до завершения всех этапов:** от 10 до 14 недель.