# Telegram-бот для типографии "Азарин" с AI-ассистентом

Это продвинутый Telegram-бот, разработанный на Kotlin для автоматизации консультаций в типографии "Азарин". Он использует локально развернутую большую языковую модель (LLM) для ведения осмысленного диалога с пользователями, запоминания контекста и ответов на вопросы об услугах.


## Основные возможности

*   **Диалоговый AI:** Бот способен вести естественный диалог, понимать уточняющие вопросы и помнить предыдущие реплики пользователя.
*   **Локальная LLM:** Вся обработка запросов происходит на вашем оборудовании с помощью `llama.cpp`, что обеспечивает полную приватность и независимость от внешних сервисов.
*   **Двусторонний перевод:** Для повышения качества ответов, запросы пользователей переводятся на английский язык (на котором LLM обучена лучше), а ответы модели переводятся обратно на русский. Это происходит незаметно для пользователя благодаря локальному сервису `LibreTranslate`.
*   **Асинхронная обработка:** Запросы к LLM могут занимать время. Чтобы бот не "зависал", используется система очередей, позволяющая обрабатывать тяжелые задачи в фоновом режиме.
*   **Чистая архитектура:** Проект построен на принципах разделения ответственности, что упрощает его поддержку и расширение.

---

## Как это работает: Магия больших языковых моделей

Этот бот — прекрасный пример практического применения LLM. Давайте разберемся, как текст вопроса превращается в осмысленный ответ.

### Что такое LLM?

**Большая языковая модель (LLM)** — это нейронная сеть огромного размера (миллиарды параметров), обученная на гигантских объемах текста из интернета, книг и других источников. В процессе обучения она не просто запоминает текст, а учится выявлять статистические закономерности, грамматические структуры, смысловые связи и даже логические отношения между словами и понятиями.

По своей сути, LLM — это невероятно сложный "предсказатель следующего слова".

### От промпта к ответу: пошаговый процесс

Когда вы отправляете боту сообщение, происходит следующее:

1.  **Формирование Промпта (Prompt Engineering):**
    Бот не просто отправляет ваш вопрос модели. Он конструирует специальный текстовый блок — **промпт**. В нашем боте он состоит из трех частей:
    *   **Системная инструкция (System Prompt):** Это "роль" или "личность", которую мы задаем модели. Например: _"Ты — вежливый и полезный ассистент типографии 'Азарин'. Отвечай коротко и по делу."_ Это заставляет модель генерировать ответы в нужном нам стиле и контексте.
    *   **История диалога (Context):** Бот добавляет в промпт несколько последних пар "вопрос-ответ", чтобы модель "помнила", о чем шла речь ранее.
    *   **Новый запрос пользователя (User Prompt):** Ваше последнее сообщение.

2.  **Токенизация:**
    Получив промпт, LLM не работает с ним как с цельным текстом. Она разбивает его на минимальные смысловые единицы — **токены**. Токен может быть словом, частью слова (например, "бот" и "а"), знаком препинания. Например, фраза "Привет, мир!" будет разбита на токены `["Привет", ",", " мир", "!"]`.

3.  **Генерация (Inference):**
    Это и есть магия. Модель анализирует последовательность токенов из промпта и, используя свои внутренние "знания" (веса нейронов), вычисляет вероятность того, какой токен должен идти следующим.
    *   Она генерирует самый вероятный токен (например, после "Какие у вас услуги?" токен "Мы" будет очень вероятен).
    *   Затем она добавляет этот новый токен к последовательности и повторяет процесс: анализирует уже "Какие у вас услуги? Мы" и предсказывает следующий токен ("предоставляем").
    *   Этот процесс повторяется токен за токеном, пока модель не сгенерирует специальный токен конца текста или не достигнет лимита по количеству токенов.

4.  **Обратная токенизация и постобработка:**
    Последовательность сгенерированных токенов собирается обратно в читаемый текст. Наш бот затем выполняет дополнительную очистку (убирает технические теги) и отправляет вам чистый ответ.

Таким образом, LLM не "думает" и не "понимает" в человеческом смысле. Она является невероятно мощным статистическим инструментом, который на основе контекста предсказывает наиболее подходящее продолжение текста, создавая иллюзию осмысленного диалога.

---

## Инструкция по запуску и настройке

Для запуска проекта на вашем компьютере (протестировано на Ubuntu) необходимо выполнить три основных шага: настроить локальную LLM, запустить сервис перевода и запустить самого бота.

### Шаг 1: Настройка локальной LLM (`llama.cpp`)

LLM будет работать на вашем CPU.

#### 1.1. Установка зависимостей
Откройте терминал и установите инструменты для сборки:
```bash
sudo apt update
sudo apt install -y git make g++
```

#### 1.2. Сборка `llama.cpp`
Мы скомпилируем `llama.cpp` из исходного кода, отключив все необязательные зависимости для максимальной стабильности.

```bash
# Перейдите в домашнюю директорию и скачайте исходный код
cd ~
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Очищаем предыдущие попытки сборки (на всякий случай)
rm -rf build

# Конфигурируем сборку с помощью CMake, отключая BLAS и Vulkan
cmake -B build -DGGML_BLAS=OFF -DGGML_VULKAN=OFF -DLLAMA_CURL=OFF

# Запускаем саму компиляцию
cmake --build build --config Release
```
После завершения в папке `~/llama.cpp/build/bin/` появится исполняемый файл `llama-cli`.

#### 1.3. Скачивание модели
Создадим папку для моделей и скачаем туда квантованную (сжатую) версию `Phi-3-mini`.

```bash
cd ~
mkdir llm_models
cd llm_models

# Скачиваем модель (размер ~2.2 ГБ)
wget -c "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
```

### Шаг 2: Запуск сервиса переводов (`LibreTranslate`)

Мы будем использовать Docker для простого и изолированного запуска.

#### 2.1. Установка Docker
Если Docker не установлен, выполните официальную инструкцию по установке:
```bash
# Установка необходимых пакетов
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Добавляем пользователя в группу docker, чтобы не использовать sudo
sudo usermod -aG docker ${USER}

# ВАЖНО: после этого нужно выйти из системы и войти снова.
```

#### 2.2. Запуск контейнера LibreTranslate
Запустите контейнер в фоновом режиме. Он будет автоматически стартовать вместе с системой. Мы загружаем только русские и английские языковые пакеты для экономии ресурсов.

```bash
docker run -d --name libretranslate --restart always -p 5000:5000 libretranslate/libretranslate --load-only "en,ru"
```
Чтобы убедиться, что сервис работает, откройте в браузере `http://localhost:5000`.

### Шаг 3: Настройка и запуск проекта

#### 3.1. Клонирование репозитория
```bash
git clone https://github.com/khnychenkoav/TelegramTypographyV2.0.git
cd TelegramTypographyV2.0
```

#### 3.2. Настройка путей и токена
Откройте проект в IntelliJ IDEA. Вам нужно отредактировать два файла:

1.  **`src/main/kotlin/utils/Constants.kt`**:
    Вставьте сюда токен вашего Telegram-бота.
    ```kotlin
    const val BOT_TOKEN = "ВАШ_ТЕЛЕГРАМ_ТОКЕН"
    ```

2.  **`src/main/kotlin/Main.kt`**:
    Укажите абсолютные пути к `llama-cli` и файлу модели, которые вы настроили на Шаге 1.
    ```kotlin
    // ...
    val llamaBinaryPath = "/home/имя_пользователя/llama.cpp/build/bin/llama-cli"
    val modelPath = "/home/имя_пользователя/llm_models/Phi-3-mini-4k-instruct-q4.gguf"
    // ...
    ```

#### 3.3. Запуск
Откройте файл `src/main/kotlin/Main.kt` и нажмите на зеленую стрелку "Run" рядом с функцией `main()`.

После инициализации бот будет готов к работе в Telegram.


---

## Полный план работ

**Статус проекта:** `Этап 0 завершен.`

---

### ✅ Этап 0: Создание архитектуры и базовой интеграции - `Выполнено`

*   **[✔] Задача 0.1: Настройка проекта и базовое подключение к Telegram**
    *   **Описание:** Инициализация Gradle-проекта, подключение библиотеки для Telegram-бота, создание первой команды `/start` для проверки работоспособности.
    *   **Срок:** `~1 день`.
    *   **Результат:** Бот отвечает на команды.

*   **[✔] Задача 0.2: Построение чистой архитектуры**
    *   **Описание:** Разделение логики на слои: `Bot` (взаимодействие с API), `ResponseHandler` (логика ответов), `state` (управление состоянием). Это предотвращает превращение кода в "спагетти".
    *   **Срок:** `~2 дня`.
    *   **Результат:** Код легко читать, поддерживать и расширять.

*   **[✔] Задача 0.3: Реализация управления состоянием (сессиями)**
    *   **Описание:** Создание `SessionManager` и `UserSession` для хранения информации о каждом пользователе (имя, текущий режим диалога). Бот научился "узнавать" пользователя и вести персонализированный диалог.
    *   **Срок:** `~2 дня`.
    *   **Результат:** Бот помнит пользователя и контекст.

*   **[✔] Задача 0.4: Внедрение системы ресурсов и навигации**
    *   **Описание:** Вынесение всего текста (сообщения, надписи на кнопках) в файл `resources/messages_ru.properties`. Создание `KeyboardFactory` для управления Inline-клавиатурами.
    *   **Срок:** `~2-3 дня` (включая отладку кодировок).
    *   **Результат:** Тексты легко редактировать, код чистый, у пользователя есть удобная навигация.

*   **[✔] Задача 0.5: Настройка и интеграция локальной LLM и сервиса переводов**
    *   **Описание:** Компиляция `llama.cpp` на Ubuntu, скачивание модели. Развертывание `LibreTranslate` через Docker. Создание сервисного слоя (`LlmService`, `TranslationService`) и асинхронной очереди (`JobQueue`). Отладка взаимодействия Java-процесса с нативной утилитой `llama-cli`.
    *   **Срок:** `~5-7 дней` (самый сложный и непредсказуемый этап).
    *   **Результат:** Бот способен генерировать осмысленные ответы с помощью локальной LLM.

*   **[✔] Задача 0.6: Реализация диалога с памятью и переводом**
    *   **Описание:** Интеграция `TranslationService` в `LlmService`. Реализация логики сохранения истории диалога в `UserSession` и ее использования для формирования контекста для LLM.
    *   **Срок:** `~2-3 дня`.
    *   **Результат:** Бот ведет полноценный диалог, помнит контекст и отвечает на более качественном, переведенном языке.

---

### ⏳ Этап 1: Фундаментальный функционал (MVP) - `Начат, ~3-4 недели`

На этом этапе доводим до ума самые важные для бизнеса функции.

*   **[ ] Задача 1.1: Продвинутая система калькуляции заказов**
    *   **Описание:** Создание гибридной системы, где AI извлекает параметры, а жесткая логика по прайс-листам производит точный расчет.
    *   **[✔] Подзадача 1.1.1:** Создать структуру прайс-листов (JSON) в `resources` для всех основных продуктов.
    *   **[✔] Подзадача 1.1.2:** Реализовать `PriceListProvider` для загрузки прайсов.
    *   **[✔] Подзадача 1.1.3:** Реализовать `CalculatorService` с логикой расчета для 2-3 ключевых продуктов.
    *   **[ ] Подзадача 1.1.4:** Создать и отладить два системных промпта: "Экстрактор" (текст -> JSON) и "Советник" (расчет -> человеческий ответ с советом).
    *   **[ ] Подзадача 1.1.5:** Реализовать многоступенчатую логику в `ResponseHandler` для режима калькуляции.
    *   **Примерный срок:** `10-14 дней`. Это самая объемная и важная задача.

*   **[ ] Задача 1.2: Надежная связь с оператором**
    *   **Описание:** Реализация функции "Связаться с оператором", которая пересылает сообщение пользователя в специальный чат.
    *   **[ ] Подзадача 1.2.1:** Настроить ID чата операторов.
    *   **[ ] Подзадача 1.2.2:** Реализовать форматирование и пересылку сообщения в `ResponseHandler`.
    *   **Примерный срок:** `2 дня`.

*   **[ ] Задача 1.3: Интеграция и выбор удаленной LLM (GigaChat)**
    *   **Описание:** Добавление возможности использовать более мощную облачную модель для сложных задач.
    *   **[ ] Подзадача 1.3.1:** Добавить зависимости GigaChat SDK.
    *   **[ ] Подзадача 1.3.2:** Создать `GigaChatServiceImpl.kt`.
    *   **[ ] Подзадача 1.3.3:** Реализовать в интерфейсе бота (кнопки, команды) возможность переключения между локальной и удаленной моделью.
    *   **Примерный срок:** `4-5 дней`.

---

### ⏳ Этап 2: Улучшение пользовательского опыта (UX/UI) - `Предстоит, ~2 недели`

Делаем взаимодействие с ботом приятным и интуитивным.

*   **[ ] Задача 2.1: Визуальное обогащение ответов**
    *   **Описание:** Добавление изображений, Markdown-разметки и анимаций.
    *   **Примерный срок:** `3 дня`.

*   **[ ] Задача 2.2: "Приборка" в чате**
    *   **Описание:** Реализация удаления/редактирования старых сообщений и клавиатур для поддержания чистоты в диалоге.
    *   **Примерный срок:** `3 дня`.

*   **[ ] Задача 2.3: Расширение навигации и информационных разделов**
    *   **Описание:** Добавление кнопок и обработчиков для статических информационных разделов ("Адреса", "Требования к макетам" и т.д.).
    *   **Примерный срок:** `2 дня`.

---

### ⏳ Этап 3: Продвинутые AI-возможности - `Предстоит, ~4-5 недель`

Выходим за рамки простого ассистента.

*   **[ ] Задача 3.1: Обработка файлов и изображений**
    *   **Описание:** Научить бота принимать файлы (макеты) и изображения, пересылать их оператору. В перспективе — анализ изображений мультимодальной LLM.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 3.2: Интеграция с "интернетом" (поиск актуальной информации)**
    *   **Описание:** Дать боту возможность искать информацию в интернете для ответов на вопросы о трендах, новых материалах и т.д.
    *   **Примерный срок:** `7-10 дней`.

*   **[ ] Задача 3.3: Реализация пошагового брифа на дизайн**
    *   **Описание:** Создание интерактивного опросника, который проведет пользователя по всем вопросам из вашего брифа.
    *   **Примерный срок:** `5-7 дней`.

---

### ⏳ Этап 4: Администрирование и развертывание - `Предстоит, ~2 недели`

Подготовка к реальному использованию.

*   **[ ] Задача 4.1: Режим для сотрудников**
    *   **Описание:** Создание панели администратора с командами для просмотра статистики, логов и отправки массовых рассылок.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 4.2: Подготовка к развертыванию (Deployment)**
    *   **Описание:** Написание Dockerfile для всего приложения, настройка CI/CD для автоматической сборки и развертывания на сервере.
    *   **Примерный срок:** `3-5 дней`.

---

**Общий примерный срок до завершения всех этапов:** от 10 до 14 недель.

