
# Telegram-бот для типографии "Азарин" с AI-ассистентом

![slon.png](src/main/resources/img/slon.png)

Это продвинутый Telegram-бот, разработанный на Kotlin для автоматизации консультаций в типографии "Азарин". Он использует локально развернутую большую языковую модель (LLM) для ведения осмысленного диалога с пользователями, а также предоставляет надежный пошаговый калькулятор для расчета стоимости продукции.


## Основные возможности

*   **Гибридный AI-ассистент:** Бот способен вести естественный диалог с пользователем, понимать уточняющие вопросы и помнить контекст беседы благодаря интеграции с `GigaChat` и локальной LLM.
*   **Надежный калькулятор:** Реализован пошаговый калькулятор с кнопками, который позволяет точно рассчитать стоимость стандартной продукции (значки, цифровая печать, резка материалов) на основе актуальных прайс-листов в формате JSON.
*   **Переключаемые LLM:** Система позволяет "на лету" переключаться между мощной облачной моделью (`GigaChat`) для сложных задач и полностью приватной локальной моделью (`llama.cpp`) для простых запросов.
*   **Административные функции:** Реализована защищенная панель для операторов, позволяющая выполнять массовую рассылку сообщений всем пользователям бота.
*   **Асинхронность и надежность:** Запросы к AI обрабатываются в фоновой очереди, что обеспечивает отзывчивость интерфейса. Бот защищен от сбоев внешних сервисов благодаря таймаутам и грамотной обработке ошибок.
*   **Чистая архитектура:** Проект построен на принципах разделения ответственности, что упрощает его поддержку, тестирование и расширение.

---

## Как это работает: Магия больших языковых моделей

Этот бот — прекрасный пример практического применения LLM. Давайте разберемся, как текст вопроса превращается в осмысленный ответ.

### Что такое LLM?

**Большая языковая модель (LLM)** — это нейронная сеть огромного размера (миллиарды параметров), обученная на гигантских объемах текста из интернета, книг и других источников. В процессе обучения она не просто запоминает текст, а учится выявлять статистические закономерности, грамматические структуры, смысловые связи и даже логические отношения между словами и понятиями.

По своей сути, LLM — это невероятно сложный "предсказатель следующего слова".

### От промпта к ответу: пошаговый процесс
Когда пользователь отправляет сообщение, оно запускает следующую цепочку событий:

1.  **`TypographyBot` (Точка входа):** Принимает `update` от Telegram API. С помощью диспетчера определяет тип события (команда, текст, нажатие кнопки) и передает управление в `ResponseHandler`.

2.  **`ResponseHandler` (Контроллер):** "Мозговой центр" бота. Он обращается к `SessionManager`, чтобы понять, в каком состоянии (`UserMode`) находится пользователь, и решает, что делать дальше:
    *   **Простой вопрос?** → Отправить в `CannedResponses` или `ComplexityAnalyzer`.
    *   **Запрос на расчет?** → Запустить пошаговый сценарий калькулятора.
    *   **Сообщение в чате с AI?** → Поставить задачу в `JobQueue`.

3.  **`JobQueue` (Асинхронный обработчик):** Тяжелые задачи, такие как запрос к LLM, не выполняются немедленно. Они помещаются в очередь. Специальный воркер в фоновом режиме забирает задачу и выполняет ее, не блокируя основной поток. Это гарантирует, что бот всегда останется отзывчивым.

4.  **`LlmSwitcher` и `LlmService` (Сервисный слой):** Воркер через `LlmSwitcher` определяет, какую модель использовать (локальную или `GigaChat`), и вызывает метод `generateWithHistory` у соответствующей реализации `LlmService`.

5.  **Генерация ответа и возврат:** После получения ответа от LLM, воркер вызывает callback-функцию, которая через `ResponseHandler` отправляет отформатированное и очищенное сообщение обратно пользователю.

### Магия больших языковых моделей: от промпта до ответа

Когда запрос доходит до `LlmService`, происходит самое интересное.

1.  **Анализ сложности и выбор модели:**
    *   `ComplexityAnalyzer` проверяет текст на наличие "сложных" ключевых слов ("посоветуй", "сравни", "идея").
    *   Если запрос простой (например, "какой у вас адрес?"), `LlmSwitcher` активирует быструю и приватную **локальную модель** (`LlamaCliServiceImpl`).
    *   Если запрос требует креативности или глубоких знаний, активируется мощная облачная модель **`GigaChat`** (`GigaChatServiceImpl`).

2.  **Инженерный промптинг (Prompt Engineering):**
    Просто отправить текст пользователя модели — неэффективно. Мы конструируем специальный, "инженерный" промпт, который направляет модель в нужное русло. В нашем боте он состоит из трех частей:
    *   **Системная инструкция (`System Prompt`):** Это "личность" и "база знаний" нашего AI. Мы "приказываем" модели: _"Ты — профессиональный консультант типографии 'Азарин' в Новосибирске. Вот наши адреса, телефоны, список услуг..."_ Этот текст из `messages_ru.properties` заставляет модель генерировать ответы в нужном стиле и контексте, используя предоставленные данные.
    *   **История диалога (`Context`):** `SessionManager` хранит последние несколько пар "вопрос-ответ". Мы добавляем их в промпт, чтобы модель "помнила", о чем шла речь ранее. Это позволяет пользователю задавать уточняющие вопросы вроде "А сколько будет стоить доставка?".
    *   **Новый запрос пользователя (`User Prompt`):** И, наконец, само сообщение пользователя.

3.  **Токенизация и генерация (Inference):**
    *   Собранный промпт разбивается на минимальные смысловые единицы — **токены** (слова, части слов, знаки препинания).
    *   LLM анализирует эту последовательность токенов и, используя свои внутренние "знания" (миллиарды параметров-весов), вычисляет наиболее вероятный следующий токен.
    *   Этот процесс повторяется циклически: сгенерированный токен добавляется к последовательности, и модель предсказывает следующий. Так, слово за словом, рождается осмысленный ответ.

4.  **Постобработка и санитайзинг:**
    *   Последовательность токенов собирается обратно в текст.
    *   **Это важный шаг:** LLM часто возвращает текст с Markdown-разметкой (`*жирный*`, `_курсив_`), но иногда "забывает" закрыть теги. Наш `Sanitizer` проходит по тексту, исправляет некорректную разметку и экранирует "опасные" символы. Это защищает нас от ошибок Telegram API (`Bad Request: can't parse entities`).
    *   Только после этой очистки финальный, безопасный и красивый ответ отправляется пользователю.

Таким образом, бот не просто "думает". Он выполняет сложный, многоступенчатый процесс, сочетая жесткую логику, управление состоянием и мощь статистического анализа больших языковых моделей для создания иллюзии осмысленного диалога.

---

## Инструкция по запуску и настройке

Для запуска проекта на вашем компьютере (протестировано на Ubuntu) необходимо выполнить три основных шага: настроить локальную LLM, запустить сервис перевода и запустить самого бота.

### Шаг 1: Настройка локальной LLM (`llama.cpp`)

LLM будет работать на вашем CPU.

#### 1.1. Установка зависимостей
Откройте терминал и установите инструменты для сборки:
```bash
sudo apt update
sudo apt install -y git make g++
```

#### 1.2. Сборка `llama.cpp`
Мы скомпилируем `llama.cpp` из исходного кода, отключив все необязательные зависимости для максимальной стабильности.

```bash
# Перейдите в домашнюю директорию и скачайте исходный код
cd ~
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Очищаем предыдущие попытки сборки (на всякий случай)
rm -rf build

# Конфигурируем сборку с помощью CMake, отключая BLAS и Vulkan
cmake -B build -DGGML_BLAS=OFF -DGGML_VULKAN=OFF -DLLAMA_CURL=OFF

# Запускаем саму компиляцию
cmake --build build --config Release
```
После завершения в папке `~/llama.cpp/build/bin/` появится исполняемый файл `llama-cli`.

#### 1.3. Скачивание модели
Создадим папку для моделей и скачаем туда квантованную (сжатую) версию `Phi-3-mini`.

```bash
cd ~
mkdir llm_models
cd llm_models

# Скачиваем модель (размер ~2.2 ГБ)
wget -c "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
```

### Шаг 2: Запуск сервиса переводов (`LibreTranslate`)

Мы будем использовать Docker для простого и изолированного запуска.

#### 2.1. Установка Docker
Если Docker не установлен, выполните официальную инструкцию по установке:
```bash
# Установка необходимых пакетов
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Добавляем пользователя в группу docker, чтобы не использовать sudo
sudo usermod -aG docker ${USER}

# ВАЖНО: после этого нужно выйти из системы и войти снова.
```

#### 2.2. Запуск контейнера LibreTranslate
Запустите контейнер в фоновом режиме. Он будет автоматически стартовать вместе с системой. Мы загружаем только русские и английские языковые пакеты для экономии ресурсов.

```bash
docker run -d --name libretranslate --restart always -p 5000:5000 libretranslate/libretranslate --load-only "en,ru"
```
Чтобы убедиться, что сервис работает, откройте в браузере `http://localhost:5000`.

### Шаг 3: Настройка и запуск проекта

#### 3.1. Клонирование репозитория
```bash
git clone https://github.com/khnychenkoav/TelegramTypographyV2.0.git
cd TelegramTypographyV2.0
```

#### 3.2. Настройка путей и токена
Откройте проект в IntelliJ IDEA. Вам нужно отредактировать два файла:

1.  **`src/main/kotlin/utils/Constants.kt`**:
    Вставьте сюда токен вашего Telegram-бота.
    ```kotlin
    const val BOT_TOKEN = "ВАШ_ТЕЛЕГРАМ_ТОКЕН"
    ```

2.  **`src/main/kotlin/Main.kt`**:
    Укажите абсолютные пути к `llama-cli` и файлу модели, которые вы настроили на Шаге 1.
    ```kotlin
    // ...
    val llamaBinaryPath = "/home/имя_пользователя/llama.cpp/build/bin/llama-cli"
    val modelPath = "/home/имя_пользователя/llm_models/Phi-3-mini-4k-instruct-q4.gguf"
    // ...
    ```

#### 3.3. Запуск
Откройте файл `src/main/kotlin/Main.kt` и нажмите на зеленую стрелку "Run" рядом с функцией `main()`.

После инициализации бот будет готов к работе в Telegram.


---

## Полный план работ

**Статус проекта:** `Этап 1.1 завершен.`

---

### ✅ Этап 0: Базовая архитектура и интеграции - `Выполнено`

*   **[✔] Задача 0.1:** Настройка проекта, Gradle, CI/CD.
*   **[✔] Задача 0.2:** Интеграция с Telegram Bot API, создание базового диспетчера.
*   **[✔] Задача 0.3:** Реализация управления сессиями пользователей.
*   **[✔] Задача 0.4:** Интеграция с локальной LLM (`llama.cpp`) и облачной (`GigaChat`) через общий интерфейс `LlmService`.
*   **[✔] Задача 0.5:** Реализация асинхронной очереди `JobQueue` для обработки тяжелых запросов.

---

### ✅ Этап 1: Фундаментальный функционал (MVP) - `Выполнено`

На этом этапе доводим до ума самые важные для бизнеса функции.

*   **[✔] Задача 1.1: Надежный кнопочный калькулятор - `Выполнено`**
    *   **Описание:** Реализована система точного расчета стоимости по прайс-листам. Пользователь пошагово выбирает параметры заказа с помощью кнопок.
    *   **[✔] Подзадача 1.1.1:** Создана и отлажена структура прайс-листов (JSON) для всех основных продуктов.
    *   **[✔] Подзадача 1.1.2:** Реализован `PriceListProvider` для загрузки и парсинга прайсов.
    *   **[✔] Подзадача 1.1.3:** Реализован `CalculatorService` с надежной логикой расчета для всех заявленных продуктов (значки, цифровая печать, резка, резка с печатью).
    *   **[✔] Подзадача 1.1.4:** Реализована полная и корректная пошаговая логика в `ResponseHandler` для всех сценариев калькулятора.
    *   **Примерный срок:** `10-14 дней`.

*   **[✔] Задача 1.2: Интеграция LLM в калькулятор - `Предстоит`**
    *   **Описание:** Создание гибридной системы, где AI помогает пользователю, а жесткая логика по-прежнему отвечает за точность.
    *   **[✔] Подзадача 1.2.1:** Реализовать режим **"LLM-Анализатор"**: пользователь описывает заказ текстом, а LLM выдает приблизительный расчет (нестабильно).
    *   **[✔] Подзадача 1.2.2:** Реализовать режим **"LLM-Комментатор"**: после получения точного расчета от `CalculatorService`, LLM "оборачивает" его в красивый, человечный ответ с полезными советами.
    *   **Примерный срок:** `5-7 дней`.

*   **[✔] Задача 1.3: Надежная связь с оператором**
    *   **Описание:** Реализация функции "Связаться с оператором", которая пересылает сообщение пользователя в специальный чат.
    *   **[✔] Подзадача 1.3.1:** Настроить ID чата операторов.
    *   **[✔] Подзадача 1.3.2:** Реализовать форматирование и пересылку сообщения в `ResponseHandler`.
    *   **Примерный срок:** `2 дня`.

*   **[✔] Задача 1.4: Интеграция и выбор удаленной LLM (GigaChat)**
    *   **Описание:** Добавление возможности использовать более мощную облачную модель для сложных задач.
    *   **[✔] Подзадача 1.4.1:** Добавить зависимости GigaChat SDK.
    *   **[✔] Подзадача 1.4.2:** Создать `GigaChatServiceImpl.kt`.
    *   **[✔] Подзадача 1.4.3:** Реализовать в интерфейсе бота (кнопки, команды) возможность переключения между локальной и удаленной моделью.
    *   **Примерный срок:** `4-5 дней`.

---

### ✅ Этап 2: Улучшение пользовательского опыта (UX/UI) - `Выполнено`

Делаем взаимодействие с ботом приятным и интуитивным.

*   **[✔] Задача 2.1: Визуальное обогащение ответов**
    *   **Описание:** Добавление изображений, Markdown-разметки и анимаций.
    *   **Примерный срок:** `3 дня`.

*   **[✔] Задача 2.2: "Приборка" в чате**
    *   **Описание:** Реализация удаления/редактирования старых сообщений и клавиатур для поддержания чистоты в диалоге.
    *   **Примерный срок:** `3 дня`.

*   **[✔] Задача 2.3: Расширение навигации и информационных разделов**
    *   **Описание:** Добавление кнопок и обработчиков для статических информационных разделов ("Адреса", "Требования к макетам" и т.д.).
    *   **Примерный срок:** `2 дня`.

---

### ✅ Этап 3: Продвинутые AI-возможности - `Частично выполнен`

Выходим за рамки простого ассистента.

*   **[✔] Задача 3.1: Обработка файлов и изображений**
    *   **Описание:** Научить бота принимать файлы (макеты) и изображения, пересылать их оператору. В перспективе — анализ изображений мультимодальной LLM.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 3.2: Интеграция с "интернетом" (поиск актуальной информации)**
    *   **Описание:** Дать боту возможность искать информацию в интернете для ответов на вопросы о трендах, новых материалах и т.д.
    *   **Примерный срок:** `7-10 дней`.

*   **[ ] Задача 3.3: Реализация пошагового брифа на дизайн**
    *   **Описание:** Создание интерактивного опросника, который проведет пользователя по всем вопросам из вашего брифа.
    *   **Примерный срок:** `5-7 дней`.

---

### ⏳ Этап 4: Администрирование и развертывание - `Предстоит, ~2 недели`

Подготовка к реальному использованию.

*   **[ ] Задача 4.1: Режим для сотрудников**
    *   **Описание:** Создание панели администратора с командами для просмотра статистики, логов и отправки массовых рассылок.
    *   **Примерный срок:** `5-7 дней`.

*   **[ ] Задача 4.2: Подготовка к развертыванию (Deployment)**
    *   **Описание:** Написание Dockerfile для всего приложения, настройка CI/CD для автоматической сборки и развертывания на сервере.
    *   **Примерный срок:** `3-5 дней`.

*   **[✔] Задача 4.3: Повышение отказоустойчивости.** Внедрены таймауты для всех сетевых запросов. `LlmService` переведен на `suspend`-функции для полной асинхронности. Добавлена обработка ошибок внешних сервисов.
*   **[✔] Задача 4.4: Администрирование.** Реализована функция массовой рассылки сообщений всем пользователям через команду `/news_message` в чате операторов.
*   **[✔] Задача 4.5: Улучшение UX.** Внедрен санитайзер Markdown для предотвращения ошибок API. Улучшена навигация и информационные разделы.


---

**Общий примерный срок до завершения всех этапов:** от 3 до 5 недель.