# Telegram-бот для типографии "Азарин" с AI-ассистентом

Это продвинутый Telegram-бот, разработанный на Kotlin для автоматизации консультаций в типографии "Азарин". Он использует локально развернутую большую языковую модель (LLM) для ведения осмысленного диалога с пользователями, запоминания контекста и ответов на вопросы об услугах.


## Основные возможности

*   **Диалоговый AI:** Бот способен вести естественный диалог, понимать уточняющие вопросы и помнить предыдущие реплики пользователя.
*   **Локальная LLM:** Вся обработка запросов происходит на вашем оборудовании с помощью `llama.cpp`, что обеспечивает полную приватность и независимость от внешних сервисов.
*   **Двусторонний перевод:** Для повышения качества ответов, запросы пользователей переводятся на английский язык (на котором LLM обучена лучше), а ответы модели переводятся обратно на русский. Это происходит незаметно для пользователя благодаря локальному сервису `LibreTranslate`.
*   **Асинхронная обработка:** Запросы к LLM могут занимать время. Чтобы бот не "зависал", используется система очередей, позволяющая обрабатывать тяжелые задачи в фоновом режиме.
*   **Чистая архитектура:** Проект построен на принципах разделения ответственности, что упрощает его поддержку и расширение.

---

## Как это работает: Магия больших языковых моделей

Этот бот — прекрасный пример практического применения LLM. Давайте разберемся, как текст вопроса превращается в осмысленный ответ.

### Что такое LLM?

**Большая языковая модель (LLM)** — это нейронная сеть огромного размера (миллиарды параметров), обученная на гигантских объемах текста из интернета, книг и других источников. В процессе обучения она не просто запоминает текст, а учится выявлять статистические закономерности, грамматические структуры, смысловые связи и даже логические отношения между словами и понятиями.

По своей сути, LLM — это невероятно сложный "предсказатель следующего слова".

### От промпта к ответу: пошаговый процесс

Когда вы отправляете боту сообщение, происходит следующее:

1.  **Формирование Промпта (Prompt Engineering):**
    Бот не просто отправляет ваш вопрос модели. Он конструирует специальный текстовый блок — **промпт**. В нашем боте он состоит из трех частей:
    *   **Системная инструкция (System Prompt):** Это "роль" или "личность", которую мы задаем модели. Например: _"Ты — вежливый и полезный ассистент типографии 'Азарин'. Отвечай коротко и по делу."_ Это заставляет модель генерировать ответы в нужном нам стиле и контексте.
    *   **История диалога (Context):** Бот добавляет в промпт несколько последних пар "вопрос-ответ", чтобы модель "помнила", о чем шла речь ранее.
    *   **Новый запрос пользователя (User Prompt):** Ваше последнее сообщение.

2.  **Токенизация:**
    Получив промпт, LLM не работает с ним как с цельным текстом. Она разбивает его на минимальные смысловые единицы — **токены**. Токен может быть словом, частью слова (например, "бот" и "а"), знаком препинания. Например, фраза "Привет, мир!" будет разбита на токены `["Привет", ",", " мир", "!"]`.

3.  **Генерация (Inference):**
    Это и есть магия. Модель анализирует последовательность токенов из промпта и, используя свои внутренние "знания" (веса нейронов), вычисляет вероятность того, какой токен должен идти следующим.
    *   Она генерирует самый вероятный токен (например, после "Какие у вас услуги?" токен "Мы" будет очень вероятен).
    *   Затем она добавляет этот новый токен к последовательности и повторяет процесс: анализирует уже "Какие у вас услуги? Мы" и предсказывает следующий токен ("предоставляем").
    *   Этот процесс повторяется токен за токеном, пока модель не сгенерирует специальный токен конца текста или не достигнет лимита по количеству токенов.

4.  **Обратная токенизация и постобработка:**
    Последовательность сгенерированных токенов собирается обратно в читаемый текст. Наш бот затем выполняет дополнительную очистку (убирает технические теги) и отправляет вам чистый ответ.

Таким образом, LLM не "думает" и не "понимает" в человеческом смысле. Она является невероятно мощным статистическим инструментом, который на основе контекста предсказывает наиболее подходящее продолжение текста, создавая иллюзию осмысленного диалога.

---

## Инструкция по запуску и настройке

Для запуска проекта на вашем компьютере (протестировано на Ubuntu) необходимо выполнить три основных шага: настроить локальную LLM, запустить сервис перевода и запустить самого бота.

### Шаг 1: Настройка локальной LLM (`llama.cpp`)

LLM будет работать на вашем CPU.

#### 1.1. Установка зависимостей
Откройте терминал и установите инструменты для сборки:
```bash
sudo apt update
sudo apt install -y git make g++
```

#### 1.2. Сборка `llama.cpp`
Мы скомпилируем `llama.cpp` из исходного кода, отключив все необязательные зависимости для максимальной стабильности.

```bash
# Перейдите в домашнюю директорию и скачайте исходный код
cd ~
git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp

# Очищаем предыдущие попытки сборки (на всякий случай)
rm -rf build

# Конфигурируем сборку с помощью CMake, отключая BLAS и Vulkan
cmake -B build -DGGML_BLAS=OFF -DGGML_VULKAN=OFF -DLLAMA_CURL=OFF

# Запускаем саму компиляцию
cmake --build build --config Release
```
После завершения в папке `~/llama.cpp/build/bin/` появится исполняемый файл `llama-cli`.

#### 1.3. Скачивание модели
Создадим папку для моделей и скачаем туда квантованную (сжатую) версию `Phi-3-mini`.

```bash
cd ~
mkdir llm_models
cd llm_models

# Скачиваем модель (размер ~2.2 ГБ)
wget -c "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf"
```

### Шаг 2: Запуск сервиса переводов (`LibreTranslate`)

Мы будем использовать Docker для простого и изолированного запуска.

#### 2.1. Установка Docker
Если Docker не установлен, выполните официальную инструкцию по установке:
```bash
# Установка необходимых пакетов
sudo apt install -y apt-transport-https ca-certificates curl software-properties-common
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io

# Добавляем пользователя в группу docker, чтобы не использовать sudo
sudo usermod -aG docker ${USER}

# ВАЖНО: после этого нужно выйти из системы и войти снова.
```

#### 2.2. Запуск контейнера LibreTranslate
Запустите контейнер в фоновом режиме. Он будет автоматически стартовать вместе с системой. Мы загружаем только русские и английские языковые пакеты для экономии ресурсов.

```bash
docker run -d --name libretranslate --restart always -p 5000:5000 libretranslate/libretranslate --load-only "en,ru"
```
Чтобы убедиться, что сервис работает, откройте в браузере `http://localhost:5000`.

### Шаг 3: Настройка и запуск проекта

#### 3.1. Клонирование репозитория
```bash
git clone https://github.com/khnychenkoav/TelegramTypographyV2.0.git
cd TelegramTypographyV2.0
```

#### 3.2. Настройка путей и токена
Откройте проект в IntelliJ IDEA. Вам нужно отредактировать два файла:

1.  **`src/main/kotlin/utils/Constants.kt`**:
    Вставьте сюда токен вашего Telegram-бота.
    ```kotlin
    const val BOT_TOKEN = "ВАШ_ТЕЛЕГРАМ_ТОКЕН"
    ```

2.  **`src/main/kotlin/Main.kt`**:
    Укажите абсолютные пути к `llama-cli` и файлу модели, которые вы настроили на Шаге 1.
    ```kotlin
    // ...
    val llamaBinaryPath = "/home/имя_пользователя/llama.cpp/build/bin/llama-cli"
    val modelPath = "/home/имя_пользователя/llm_models/Phi-3-mini-4k-instruct-q4.gguf"
    // ...
    ```

#### 3.3. Запуск
Откройте файл `src/main/kotlin/Main.kt` и нажмите на зеленую стрелку "Run" рядом с функцией `main()`.

После инициализации бот будет готов к работе в Telegram.

